{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2752c3a",
   "metadata": {},
   "source": [
    "# AML Challenge: Code Submission\n",
    "\n",
    "**Group: Golden retrieval**\n",
    "* Valeria Avino (ID: `1905974`)\n",
    "* Xavier Del Giudice (ID: `1967219`)\n",
    "* Gabriel Pinos (ID: `1965035`)\n",
    "* Leonardo Rocci (ID: `1922496`)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Notebook Description\n",
    "\n",
    "This notebook presents the code and configuration that generated one of our best-performing submissions.\n",
    "\n",
    "This notebook will:\n",
    "1.  Import all necessary libraries and custom modules from our `/src` project structure.\n",
    "2.  Define the main `run_experiment` function that encapsulates the entire pipeline.\n",
    "3.  Define the final, optimized `CONFIG` dictionary containing the best hyperparameters.\n",
    "4.  Execute the experiment to train the model and generate the final `submission.csv`.\n",
    "5.  Load our Optuna study database to demonstrate the hyperparameter tuning process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298c960a",
   "metadata": {},
   "source": [
    "## 2. The Experiment Pipeline\n",
    "\n",
    "We define a single function, `run_experiment`, which encapsulates the entire end-to-end process. This function is highly configurable via the `CONFIG` dictionary, allowing us to control every aspect of the pipeline.\n",
    "\n",
    "The main components and their configurable features are:\n",
    "\n",
    "### `EmbeddingDataModule`\n",
    "Our custom data module handles all data loading, preprocessing, and sampling.\n",
    "* **Data Augmentation:** It supports loading an additional training dataset (`coco_npz_path`) to be merged and shuffled with the original Flickr30 data.\n",
    "    * *Note: The datasets we load have been **pre-cleaned by us** to remove any training captions (and their associated images) that matched captions in the test set, preventing data leakage.*\n",
    "* **Flexible Sampling:** We can control the `batch_size` for training and the `val_mrr_batch_size` for validation (which defaults to 100 as per Kaggle's evaluation).\n",
    "* **Advanced Training Samplers:** The module allows for:\n",
    "    * `group_captions`: Ensures all captions for a single image are grouped into the same batch.\n",
    "    * `sample_hard_images`: Uses a pre-computed Faiss index to sample the hardest images for the batch, improving training efficiency.\n",
    "* **Rich Validation:** We can control the validation strategy with `val_mrr_folds` (how many batches to use) and `val_num_caption_views` (1-5), logging the overall MRR and the MRR for each view separately.\n",
    "* **Feature Engineering:** Includes an option to use `num_anchors` to compute and include relative embeddings as input features.\n",
    "* **Optional Standardization:** Implements **optional** automatic standardization of training data and de-standardization for validation and test predictions.\n",
    "\n",
    "### `MlpConnector` (Model)\n",
    "The model architecture is fully defined by the config.\n",
    "* **Architecture:** We can set all architectural parameters, such as `input_dim`, `hidden_dim`, `output_dim`, and the `activation_fun` (e.g., `torch.nn.SiLU`).\n",
    "* **Hyperparameters:** We also control `dropout_rate` and whether to `normalize_out`.\n",
    "\n",
    "### `TranslationTrainer` (Lightning Module)\n",
    "Our Lightning module wraps the model and contains the core training logic.\n",
    "* **Flexible Losses:** It can use different loss functions (like `Triplet` or `InfoNCE`), which are applied directly to a similarity matrix. The loss computation is highly configurable:\n",
    "    * **`multi_positive`**: Can be set to handle multiple positive examples (correct captions for one image) within the loss calculation.\n",
    "    * **`symmetric`**: Can be set to compute the loss on the transposed similarity matrix as well, effectively adding the inverse task (image-to-caption) to the training objective.\n",
    "* **Similarity Metric:** The similarity matrix itself is configurable, supporting standard `dot` product or a kernel-based similarity (with an `alpha` params).\n",
    "* **Dynamic Schedulers:** We implemented custom schedulers for:\n",
    "    * `BatchModeScheduler`: To dynamically change the batch composition (e.g., start with easy samples and move to hard ones).\n",
    "    * `AlphaScheduler`: To anneal loss hyperparameters.\n",
    "\n",
    "Finally, the function sets up the PyTorch Lightning `Trainer`. While we used **Early Stopping** extensively during hyperparameter tuning (see Section 5), this final notebook uses a custom `StopAtEpochCallback`. This allows us to disable validation, train on the **entire dataset** (Flickr30 + COCO), and stop at the specific epoch we identified as optimal during tuning. The trainer then generates predictions using the final model checkpoint and saves the `submission.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d110f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdelgiudice-1967219\u001b[0m (\u001b[33mdelgiudice-1967219-sapienza-universit-di-roma\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "# Environment setup and imports\n",
    "import torch\n",
    "import wandb\n",
    "import pytorch_lightning as pl\n",
    "from dotenv import load_dotenv\n",
    "import importlib\n",
    "from src import utils, data, models, trainer, losses\n",
    "\n",
    "# Reload modules in case they were edited during development\n",
    "importlib.reload(utils)\n",
    "importlib.reload(data)\n",
    "importlib.reload(models)\n",
    "importlib.reload(trainer)\n",
    "importlib.reload(losses)\n",
    "\n",
    "from src.utils import set_seed\n",
    "from src.data import EmbeddingDataModule\n",
    "from src.models.load_model import build_model_from_config\n",
    "from src.trainer import TranslationTrainer\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor\n",
    "from src.utils import StopAtEpochCallback, save_submission\n",
    "\n",
    "load_dotenv()\n",
    "wandb.login()\n",
    "\n",
    "\n",
    "def run_experiment(config: dict):\n",
    "    \"\"\"\n",
    "    Launch a full training + validation + submission pipeline.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Setup & Logging\n",
    "    set_seed(config[\"seed\"])\n",
    "    wandb.init(project=config[\"project_name\"], name=config[\"run_name\"], config=config)\n",
    "    wandb_logger = WandbLogger(log_model=False)\n",
    "\n",
    "    # Data\n",
    "    datamodule = EmbeddingDataModule(\n",
    "        data_path = config[\"data_path\"],\n",
    "        coco_npz_path = config[\"coco_npz_path\"],\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        seed=config[\"seed\"],\n",
    "        standardize=config.get(\"standardize\", False),\n",
    "        val_mrr_batch_size=config.get(\"val_mrr_batch_size\", 100),\n",
    "        val_mrr_folds=config.get(\"val_mrr_folds\", 15),\n",
    "        val_num_caption_views=config.get(\"val_num_caption_views\", 5),\n",
    "        num_anchors=config.get(\"num_anchors\", 0),\n",
    "        group_captions=config.get(\"group_captions\", False),\n",
    "        sample_hard_images=config.get(\"sample_hard_images\", False),\n",
    "    )\n",
    "    datamodule.setup()\n",
    "\n",
    "    # Model\n",
    "    print(f\"\\n Building model: {config['model'].get('type', 'MLPTranslator')}\")\n",
    "    model = build_model_from_config(config[\"model\"])\n",
    "\n",
    "    # Lightning Module\n",
    "    val_data = {\n",
    "        \"gallery\": datamodule.val_image_gallery,\n",
    "        \"mu_y\": datamodule.mu_y,\n",
    "        \"std_y\": datamodule.std_y,\n",
    "    }\n",
    "    alpha_scheduler = None\n",
    "    batch_scheduler  = None\n",
    "    if config.get(\"alpha_scheduler\"):\n",
    "        from src.utils import AlphaScheduler\n",
    "        alpha_scheduler = AlphaScheduler(**config[\"alpha_scheduler\"])\n",
    "    if config.get(\"batch_scheduler\"):\n",
    "        from src.utils import BatchModeScheduler\n",
    "        batch_scheduler = BatchModeScheduler(**config[\"batch_scheduler\"])\n",
    "    lightning_module = TranslationTrainer(\n",
    "        model=model,\n",
    "        config={\n",
    "            **config,\n",
    "            \"alpha_scheduler\": alpha_scheduler,\n",
    "            \"batch_scheduler\": batch_scheduler,   \n",
    "        },\n",
    "        val_data=val_data,\n",
    "    )\n",
    "\n",
    "    # Callbacks\n",
    "    # Commented out early stopping and best model checkpointing to always save last model, for reproducibility\n",
    "    # early_stopping = EarlyStopping(**config[\"early_stopping\"], verbose=True)\n",
    "    # checkpoint = ModelCheckpoint(\n",
    "    #     monitor=config[\"early_stopping\"][\"monitor\"],\n",
    "    #     mode=config[\"early_stopping\"][\"mode\"],\n",
    "    #     dirpath=\"checkpoints\",\n",
    "    #     filename=f\"{config['run_name']}-best-model\",\n",
    "    #     save_top_k=1,\n",
    "    # )\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        dirpath=\"checkpoints\",\n",
    "        save_top_k=0,\n",
    "        save_last=True\n",
    "        )\n",
    "    \n",
    "    lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "    stop_callback = StopAtEpochCallback(stop_epoch=115) # Best model at epoch 114\n",
    "    \n",
    "    # Trainer\n",
    "    trainer = pl.Trainer(\n",
    "        **config[\"trainer\"],\n",
    "        logger=wandb_logger,\n",
    "        callbacks=[checkpoint, lr_monitor, stop_callback], #early_stopping\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    print(\"Starting training.\")\n",
    "    trainer.fit(lightning_module, datamodule=datamodule)\n",
    "\n",
    "    # Prediction & Submission\n",
    "    print(\"Training finished. Generating predictions with the best model.\")\n",
    "    predictions = trainer.predict(datamodule=datamodule, ckpt_path=\"last\")\n",
    "    all_preds = torch.cat(predictions, dim=0)\n",
    "\n",
    "    submission_file = f\"submission_{config['run_name']}.csv\"\n",
    "    save_submission(datamodule.test_ids, all_preds, filename=submission_file)\n",
    "\n",
    "    print(f\"Run complete. Submission saved to {submission_file}\")\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778964f5",
   "metadata": {},
   "source": [
    "## 3. Final Configuration + Submission generation\n",
    "\n",
    "This is the `CONFIG` dictionary containing the optimized hyperparameters for our best run. These values were determined through extensive tuning (shown in section 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63e78ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    # Project info\n",
    "    \"project_name\": \"Triplet\",\n",
    "    \"run_name\": \"Golden_Submission\",\n",
    "    \"seed\": 42,\n",
    "\n",
    "    # Data \n",
    "    \"data_path\": \"data\",\n",
    "    \"coco_npz_path\": \"data/coco_25.npz\",\n",
    "    \"batch_size\": 2000,            # multiple of 5 (for grouped captions)\n",
    "    \"standardize\": False,\n",
    "    \"group_captions\": True,        # grouping captions by image\n",
    "    \"sample_hard_images\": False,\n",
    "    \n",
    "    \n",
    "    # Validation / MRR\n",
    "    \"val_mrr_batch_size\": 0,\n",
    "    \"val_mrr_folds\": 0,\n",
    "    \"val_num_caption_views\": 0,\n",
    "    \"val_log_chunks\": False,\n",
    "    \"num_anchors\": 0,\n",
    "\n",
    "    # Model\n",
    "    \"model\": {\n",
    "        \"type\": \"MlpConnector\",\n",
    "        \"input_dim\": 1024,\n",
    "        \"output_dim\": 1536,\n",
    "        \"hidden_dim\": 2048,\n",
    "        \"dropout_rate\": 0.7042707796929302,\n",
    "        \"normalize_out\": True,\n",
    "        \"activation_fun\": torch.nn.SiLU,\n",
    "    },\n",
    "\n",
    "    # Loss\n",
    "    \"loss_name\": \"triplet\",\n",
    "    \"margin\": 1.164467807751587,\n",
    "    \"similarity_type\": \"dot\",   \n",
    "    \"symmetric\": False,\n",
    "    \"multi_positive\": True,\n",
    "    \"use_mse\": True,\n",
    "    \"lambda_mse\" : 7.876500463590652,\n",
    "    \n",
    "    \n",
    "    # Optimization\n",
    "    \"optimizer\": {\n",
    "        \"lr\": 0.00030361953337802,\n",
    "        \"weight_decay\": 9.41586739869854e-06,\n",
    "    },\n",
    "\n",
    "    # Scheduler\n",
    "    \"use_lr_scheduler\": True,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "\n",
    "    # Early stopping\n",
    "    # \"early_stopping\": {\n",
    "    #     \"monitor\": \"val/mrr_overall_avg\",\n",
    "    #     \"mode\": \"max\",\n",
    "    #     \"patience\": 100,\n",
    "    # },\n",
    "\n",
    "    # Trainer\n",
    "    \"trainer\": {\n",
    "        \"max_epochs\": 200,          \n",
    "        \"accelerator\": \"auto\",\n",
    "        \"precision\": 16,\n",
    "        \"gradient_clip_val\": 1.089126334404685,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2687f2",
   "metadata": {},
   "source": [
    "## 4. Run Final Training & Prediction\n",
    "\n",
    "With the helper function and the final configuration defined, this cell executes the full pipeline and generates the submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15e11d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\xavie\\Desktop\\challenge_last\\wandb\\run-20251116_182708-uj7npczt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/delgiudice-1967219-sapienza-universit-di-roma/Triplet/runs/uj7npczt' target=\"_blank\">Golden_Submission</a></strong> to <a href='https://wandb.ai/delgiudice-1967219-sapienza-universit-di-roma/Triplet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/delgiudice-1967219-sapienza-universit-di-roma/Triplet' target=\"_blank\">https://wandb.ai/delgiudice-1967219-sapienza-universit-di-roma/Triplet</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/delgiudice-1967219-sapienza-universit-di-roma/Triplet/runs/uj7npczt' target=\"_blank\">https://wandb.ai/delgiudice-1967219-sapienza-universit-di-roma/Triplet/runs/uj7npczt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up data module...\n",
      "Loading BASE training data...\n",
      "Loading COCO training data...\n",
      "Loaded 125000 COCO captions and 25000 unique images.\n",
      "Combined datasets. Total images: 50000, Total captions: 250000\n",
      "Data split: 250000 train captions, 0 val captions.\n",
      "Building image-centric structures for training sampler...\n",
      "Built caption lookup for 50000 training images.\n",
      "Loading test data...\n",
      "Test data: 1500 captions.\n",
      "Data setup complete. MRR config -> batch_size=0, folds=0, views=0, N_used=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xavie\\Desktop\\challenge_last\\.venv\\Lib\\site-packages\\lightning_fabric\\connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4070') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "c:\\Users\\xavie\\Desktop\\challenge_last\\.venv\\Lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\xavie\\Desktop\\challenge_last\\.venv\\Lib\\site-packages\\pytorch_lightning\\utilities\\model_summary\\model_summary.py:231: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\n",
      "  | Name  | Type         | Params | Mode \n",
      "-----------------------------------------------\n",
      "0 | model | MlpConnector | 5.3 M  | train\n",
      "-----------------------------------------------\n",
      "5.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.3 M     Total params\n",
      "21.002    Total estimated model params size (MB)\n",
      "7         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Building model: MlpConnector\n",
      "Created MLPConnector: 1024 -> 2048 -> 1536\n",
      "Starting training.\n",
      "Data module already set up. Skipping redundant setup.\n",
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]Skipping mixed-equal MRR loader (need 5 views).\n",
      "                                                  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xavie\\Desktop\\challenge_last\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\xavie\\Desktop\\challenge_last\\.venv\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:106: Total length of `DataLoader` across ranks is zero. Please make sure this was your intention.\n",
      "c:\\Users\\xavie\\Desktop\\challenge_last\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GroupedImageBatchSampler for training.\n",
      "Epoch 114: 100%|██████████| 125/125 [00:03<00:00, 37.01it/s, v_num=pczt, train/loss_step=0.506, train/loss_epoch=0.525]\n",
      " StopAtEpochCallback: Reached epoch 115. Stop training.\n",
      "Epoch 114: 100%|██████████| 125/125 [00:03<00:00, 35.54it/s, v_num=pczt, train/loss_step=0.506, train/loss_epoch=0.520]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at C:\\Users\\xavie\\Desktop\\challenge_last\\checkpoints\\last-v1.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training finished. Generating predictions with the best model.\n",
      "Data module already set up. Skipping redundant setup.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at C:\\Users\\xavie\\Desktop\\challenge_last\\checkpoints\\last-v1.ckpt\n",
      "c:\\Users\\xavie\\Desktop\\challenge_last\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:433: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 498.91it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved to submission_Golden_Submission.csv (1500 rows)\n",
      "Run complete. Submission saved to submission_Golden_Submission.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▄▄▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇█</td></tr><tr><td>lr-AdamW</td><td>██████████▇▇▇▇▇▇▇▇▇▆▅▅▅▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁</td></tr><tr><td>train/loss_epoch</td><td>█▆▅▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss_step</td><td>█▇▇▅▆▄▄▃▃▄▃▄▄▃▃▃▄▃▄▃▃▃▂▃▂▃▂▃▂▂▁▂▁▁▁▂▁▂▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>114</td></tr><tr><td>lr-AdamW</td><td>0.00012</td></tr><tr><td>train/loss_epoch</td><td>0.5202</td></tr><tr><td>train/loss_step</td><td>0.54664</td></tr><tr><td>trainer/global_step</td><td>14374</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Golden_Submission</strong> at: <a href='https://wandb.ai/delgiudice-1967219-sapienza-universit-di-roma/Triplet/runs/uj7npczt' target=\"_blank\">https://wandb.ai/delgiudice-1967219-sapienza-universit-di-roma/Triplet/runs/uj7npczt</a><br> View project at: <a href='https://wandb.ai/delgiudice-1967219-sapienza-universit-di-roma/Triplet' target=\"_blank\">https://wandb.ai/delgiudice-1967219-sapienza-universit-di-roma/Triplet</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251116_182708-uj7npczt\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Launch the experiment\n",
    "run_experiment(CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677b41b8",
   "metadata": {},
   "source": [
    "### Second best submission (other hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "967619e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    # Project info\n",
    "    \"project_name\": \"Triplet\",\n",
    "    \"run_name\": \"Golden2\",\n",
    "    \"seed\": 42,\n",
    "\n",
    "    # Data \n",
    "    \"data_path\": \"data\",\n",
    "    \"coco_npz_path\": \"data/coco_25.npz\",\n",
    "    \"batch_size\": 2000,            # multiple of 5 (for grouped captions)\n",
    "    \"standardize\": False,\n",
    "    \"group_captions\": True,        # grouping captions by image\n",
    "    \"sample_hard_images\": False,\n",
    "    \n",
    "    \n",
    "    # Validation / MRR\n",
    "    \"val_mrr_batch_size\": 0,\n",
    "    \"val_mrr_folds\": 0,\n",
    "    \"val_num_caption_views\": 0,\n",
    "    \"val_log_chunks\": False,\n",
    "    \"num_anchors\": 0,\n",
    "\n",
    "    # Model\n",
    "    \"model\": {\n",
    "        \"type\": \"MlpConnector\",\n",
    "        \"input_dim\": 1024,\n",
    "        \"output_dim\": 1536,\n",
    "        \"hidden_dim\": 2048,\n",
    "        \"dropout_rate\": 0.88,\n",
    "        \"normalize_out\": True,\n",
    "        \"activation_fun\": torch.nn.SiLU,\n",
    "    },\n",
    "\n",
    "    # Loss\n",
    "    \"loss_name\": \"triplet\",\n",
    "    \"margin\": 0.883,\n",
    "    \"similarity_type\": \"dot\",   \n",
    "    \"symmetric\": False,\n",
    "    \"multi_positive\": True,\n",
    "    \"use_mse\": True,\n",
    "    \"lambda_mse\" : 3.7,\n",
    "    \n",
    "    \n",
    "    # Optimization\n",
    "    \"optimizer\": {\n",
    "        \"lr\": 0.0005048222945628928,\n",
    "        \"weight_decay\": 1.135105292535283e-05,\n",
    "    },\n",
    "\n",
    "    # Scheduler\n",
    "    \"use_lr_scheduler\": True,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "\n",
    "    # Early stopping\n",
    "    # \"early_stopping\": {\n",
    "    #     \"monitor\": \"val/mrr_overall_avg\",\n",
    "    #     \"mode\": \"max\",\n",
    "    #     \"patience\": 100,\n",
    "    # },\n",
    "\n",
    "    # Trainer\n",
    "    \"trainer\": {\n",
    "        \"max_epochs\": 200,          \n",
    "        \"accelerator\": \"auto\",\n",
    "        \"precision\": 16,\n",
    "        \"gradient_clip_val\": 0.5,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbe8dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59241d0",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Tuning (Optuna)\n",
    "\n",
    "To show our tuning process, we load the Optuna study from its SQLite database. This shows the best trial and its parameters, which informed our final `CONFIG` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e24d241a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xavie\\Desktop\\challenge_last\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded study 'tri_tuning_study_5' from 'sqlite:///tuning.db'\n",
      "Total trials: 62\n",
      "\n",
      " Best Trial\n",
      "Value (Validation Score): 0.9040225744247437\n",
      "\n",
      "Best Parameters Found:\n",
      "  lr: 0.00030361953337802\n",
      "  weight_decay: 9.41586739869854e-06\n",
      "  dropout_rate: 0.7042707796929302\n",
      "  margin: 1.164467807751587\n",
      "  lambda_mse: 7.876500463590652\n",
      "  hidden_dim: 2048\n",
      "  batch_size: 2000\n",
      "  gradient_clip_val: 1.089126334404685\n",
      "\n",
      " Top 5 Trials\n",
      "    number     value             datetime_start          datetime_complete  \\\n",
      "24      24  0.904023 2025-11-15 12:56:26.906317 2025-11-15 13:09:07.610189   \n",
      "52      52  0.903479 2025-11-15 14:22:22.030465 2025-11-15 14:31:38.113688   \n",
      "17      17  0.902143 2025-11-15 12:24:49.912719 2025-11-15 12:36:40.454871   \n",
      "53      53  0.902033 2025-11-15 14:31:38.124690 2025-11-15 14:40:24.292023   \n",
      "54      54  0.901853 2025-11-15 14:40:24.303531 2025-11-15 14:49:44.216120   \n",
      "\n",
      "                 duration  params_batch_size  params_dropout_rate  \\\n",
      "24 0 days 00:12:40.703872               2000             0.704271   \n",
      "52 0 days 00:09:16.083223               2000             0.609646   \n",
      "17 0 days 00:11:50.542152               1000             0.635054   \n",
      "53 0 days 00:08:46.167333               2000             0.603941   \n",
      "54 0 days 00:09:19.912589               2000             0.613829   \n",
      "\n",
      "    params_gradient_clip_val  params_hidden_dim  params_lambda_mse  params_lr  \\\n",
      "24                  1.089126               2048           7.876500   0.000304   \n",
      "52                  0.406787               3072           7.570937   0.000536   \n",
      "17                  0.552439               2048           6.742011   0.000349   \n",
      "53                  1.284065               3072           7.439078   0.000332   \n",
      "54                  1.280217               3072           8.311502   0.000320   \n",
      "\n",
      "    params_margin  params_weight_decay     state  \n",
      "24       1.164468             0.000009  COMPLETE  \n",
      "52       1.402988             0.000003  COMPLETE  \n",
      "17       0.971892             0.000010  COMPLETE  \n",
      "53       1.416675             0.000001  COMPLETE  \n",
      "54       1.381205             0.000001  COMPLETE  \n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "DB_NAME = \"tuning.db\" \n",
    "STUDY_NAME = \"tri_tuning_study_5\"\n",
    "DB_PATH = f\"sqlite:///{DB_NAME}\"\n",
    "\n",
    "if not os.path.exists(DB_NAME):\n",
    "    print(f\"Error: Database file not found at '{DB_NAME}'\")\n",
    "    print(\"Please make sure the .db file is in the same directory as this notebook.\")\n",
    "else:\n",
    "    try:\n",
    "        # Load the study\n",
    "        study = optuna.load_study(\n",
    "            study_name=STUDY_NAME,\n",
    "            storage=DB_PATH\n",
    "        )\n",
    "    \n",
    "        print(f\"Successfully loaded study '{STUDY_NAME}' from '{DB_PATH}'\")\n",
    "        print(f\"Total trials: {len(study.trials)}\")\n",
    "        \n",
    "        # Show the best trial\n",
    "        print(\"\\n Best Trial\")\n",
    "        best_trial = study.best_trial\n",
    "        print(f\"Value (Validation Score): {best_trial.value}\")\n",
    "        \n",
    "        print(\"\\nBest Parameters Found:\")\n",
    "        for key, value in best_trial.params.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "        # Show a DataFrame of the top 5 trials\n",
    "        print(\"\\n Top 5 Trials\")\n",
    "        df = study.trials_dataframe()\n",
    "        print(df.sort_values(by=\"value\", ascending=False).head(5))\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Could not load Optuna study. Please check DB_PATH and STUDY_NAME.\")\n",
    "        print(f\"Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
