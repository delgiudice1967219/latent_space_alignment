Setting up data module...
Loading BASE training data...
Loading COCO training data...
Loaded 125000 COCO captions and 25000 unique images.
Combined datasets. Total images: 50000, Total captions: 250000
Data split: 250000 train captions, 0 val captions.
Building image-centric structures for training sampler...
Built caption lookup for 50000 training images.
Loading test data...
Test data: 1500 captions.
Data setup complete. MRR config -> batch_size=0, folds=0, views=0, N_used=0

 Building model: MlpConnector
Created MLPConnector: 1024 -> 2048 -> 1536
C:\Users\xavie\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\lightning_fabric\connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!
Using 16bit Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
Starting training.
You are using a CUDA device ('NVIDIA GeForce RTX 4070') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
C:\Users\xavie\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\pytorch_lightning\loggers\wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
Data module already set up. Skipping redundant setup.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
C:\Users\xavie\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\pytorch_lightning\utilities\model_summary\model_summary.py:231: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.

  | Name  | Type         | Params | Mode
-----------------------------------------------
0 | model | MlpConnector | 5.3 M  | train
-----------------------------------------------
5.3 M     Trainable params
0         Non-trainable params
5.3 M     Total params
21.002    Total estimated model params size (MB)
7         Modules in train mode
0         Modules in eval mode
Skipping mixed-equal MRR loader (need 5 views).
C:\Users\xavie\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.
C:\Users\xavie\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\pytorch_lightning\utilities\data.py:106: Total length of `DataLoader` across ranks is zero. Please make sure this was your intention.
Using GroupedImageBatchSampler for training.
C:\Users\xavie\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.

 StopAtEpochCallback: Reached epoch 115. Stop training.
Training finished. Generating predictions with the best model.
Data module already set up. Skipping redundant setup.
Restoring states from the checkpoint path at C:\Users\xavie\Desktop\challenge_last\checkpoints\last.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Loaded model weights from the checkpoint at C:\Users\xavie\Desktop\challenge_last\checkpoints\last.ckpt
C:\Users\xavie\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:433: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.
Submission saved to submission_Golden_Submission.csv (1500 rows)
Run complete. Submission saved to submission_Golden_Submission.csv
